{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqQKcukJAicL",
    "outputId": "93cca008-100b-4b92-b8b6-1590f28da692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "     ---------------------------------------- 4.0/4.0 MB 16.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.9/77.9 KB ? eta 0:00:00\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "     ------------------------------------- 895.2/895.2 KB 28.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.11.6-cp38-cp38-win_amd64.whl (3.2 MB)\n",
      "     ---------------------------------------- 3.2/3.2 MB 29.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\ericw\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.5.1 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ericw\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8FRjwBo15uT",
    "outputId": "9bf1f0b0-8bf1-42c2-d0df-ff5a3a9a3f2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ericw\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DNdmQhn2D43",
    "outputId": "dc5fc76e-e86f-419b-d1ff-59ab945bf3b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.8\n",
      "IPython version      : 7.27.0\n",
      "\n",
      "numpy       : 1.21.2\n",
      "pandas      : 1.3.3\n",
      "torch       : 1.10.2\n",
      "transformers: 4.18.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk0GnvUQIdxc"
   },
   "source": [
    "### Making the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "dOpc8w_12D1f",
    "outputId": "ae5a781d-0e0e-4763-9cea-b8fe86dd6e2b"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from pylab import rcParams\n",
    "\n",
    "from torch import nn, optim\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qAICzZNo2Dyw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC-WS1hYIxkr"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jJ83RaRG2Dv8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$SPY wouldn't be surprised to see a green close</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shell's $70 Billion BG Deal Meets Shareholder ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SSH COMMUNICATIONS SECURITY CORP STOCK EXCHANG...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment\n",
       "0  According to the Finnish-Russian Chamber of Co...   neutral\n",
       "1  The Swedish buyout firm has sold its remaining...   neutral\n",
       "2    $SPY wouldn't be surprised to see a green close  positive\n",
       "3  Shell's $70 Billion BG Deal Meets Shareholder ...  negative\n",
       "4  SSH COMMUNICATIONS SECURITY CORP STOCK EXCHANG...  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/advanced_trainset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4J5-ddIc2DtI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>It 's not .</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>We have also cut our price projections for pap...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>`` The enterprise value of the Fray Bentos pul...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>Upon completion of these transactions , Metso ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>Financing of the project will come mainly from...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>The Company serves approximately 3,000 custome...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>BM4 middle layer headbox will be equipped with...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>www.countryelements.co.uk Designed by Patricia...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2721</th>\n",
       "      <td>The agreement will provide The Switch with dou...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>Finnish Suominen Corporation that makes wipes ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Coke dividend - 3%. Google dividend - 0%. For ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>Sales rose to 300.9 mln eur compared with last...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3319</th>\n",
       "      <td>Finnish bank Pohjola Bank Plc HEL : POH1S said...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>Can Christmas Save Sainsbury's plc And Tesco plc?</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>The pretax profit of the group 's life insuran...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>The money will be spread mainly over 2011 and ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>The pulp production in Finnish Kemij+ñrvi will...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>3 January 2011 - Finnish flag carrier Finnair ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3477</th>\n",
       "      <td>The business development initiatives in North ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>The new apartment block is going up very close...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence Sentiment\n",
       "670                                         It 's not .   neutral\n",
       "2417  We have also cut our price projections for pap...   neutral\n",
       "596   `` The enterprise value of the Fray Bentos pul...   neutral\n",
       "2629  Upon completion of these transactions , Metso ...   neutral\n",
       "1395  Financing of the project will come mainly from...   neutral\n",
       "151   The Company serves approximately 3,000 custome...   neutral\n",
       "990   BM4 middle layer headbox will be equipped with...   neutral\n",
       "893   www.countryelements.co.uk Designed by Patricia...   neutral\n",
       "2721  The agreement will provide The Switch with dou...  positive\n",
       "2078  Finnish Suominen Corporation that makes wipes ...   neutral\n",
       "109   Coke dividend - 3%. Google dividend - 0%. For ...   neutral\n",
       "1632  Sales rose to 300.9 mln eur compared with last...  positive\n",
       "3319  Finnish bank Pohjola Bank Plc HEL : POH1S said...   neutral\n",
       "2004  Can Christmas Save Sainsbury's plc And Tesco plc?  negative\n",
       "149   The pretax profit of the group 's life insuran...  positive\n",
       "1897  The money will be spread mainly over 2011 and ...   neutral\n",
       "120   The pulp production in Finnish Kemij+ñrvi will...  negative\n",
       "3020  3 January 2011 - Finnish flag carrier Finnair ...  positive\n",
       "3477  The business development initiatives in North ...   neutral\n",
       "2232  The new apartment block is going up very close...   neutral"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMSFcIqsdZyH"
   },
   "outputs": [],
   "source": [
    "df = df[:24000]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GluMm1Nj2DqK"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
    "    text = re.sub('\\t', ' ',  text)\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HpgvTb72wtm"
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptC13l5r25qH"
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 8, 6\n",
    "sns.countplot(df.sentiment)\n",
    "plt.xlabel('review score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2I-Du1vR3EZN"
   },
   "outputs": [],
   "source": [
    "def sentiment2label(sentiment):\n",
    "    if sentiment == \"positive\":\n",
    "        return 1\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "df['sentiment'] = df['sentiment'].apply(sentiment2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YK3jPP3i3lRw"
   },
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StOb4mAa3rgo"
   },
   "outputs": [],
   "source": [
    "class_names = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmBGGgUVJBpT"
   },
   "source": [
    "### Playing with XLNetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVWO_38_32jL"
   },
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "PRE_TRAINED_MODEL_NAME = 'xlnet-base-cased'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ckpqWd_YWgZ"
   },
   "outputs": [],
   "source": [
    "input_txt = \"India is my country. All Indians are my brothers and sisters\"\n",
    "encodings = tokenizer.encode_plus(input_txt, add_special_tokens=True, max_length=16, return_tensors='pt', return_token_type_ids=False, return_attention_mask=True, pad_to_max_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_ew5njtYWc1"
   },
   "outputs": [],
   "source": [
    "print('input_ids : ',encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEXfe39mYWK7"
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(encodings['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XR1xkLYtTurv"
   },
   "outputs": [],
   "source": [
    "type(encodings['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pp97KX-9UtMK"
   },
   "outputs": [],
   "source": [
    "attention_mask = pad_sequences(encodings['attention_mask'], maxlen=512, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Pzeyv1_UvMT"
   },
   "outputs": [],
   "source": [
    "attention_mask = attention_mask.astype(dtype = 'int64')\n",
    "attention_mask = torch.tensor(attention_mask) \n",
    "attention_mask.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kM7cxiA9RlnA"
   },
   "outputs": [],
   "source": [
    "encodings['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M12PPghlJUbg"
   },
   "source": [
    "### Checking the distribution of token lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW4_SrOw4BDy"
   },
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in df['review']:\n",
    "  tokens = tokenizer.encode(txt, max_length=512)\n",
    "  token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSxO20TU4dz5"
   },
   "outputs": [],
   "source": [
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 1024]);\n",
    "plt.xlabel('Token count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CR3rHUQR4pDE"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6Kutw4dJyUG"
   },
   "source": [
    "### Custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2NOYXcjPK4z"
   },
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "\n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "        review,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=False,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        input_ids = pad_sequences(encoding['input_ids'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
    "        input_ids = input_ids.astype(dtype = 'int64')\n",
    "        input_ids = torch.tensor(input_ids) \n",
    "\n",
    "        attention_mask = pad_sequences(encoding['attention_mask'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
    "        attention_mask = attention_mask.astype(dtype = 'int64')\n",
    "        attention_mask = torch.tensor(attention_mask)       \n",
    "\n",
    "        return {\n",
    "        'review_text': review,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "        'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYXt2AtW6iaT"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.5, random_state=101)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmAsa9pg6oac"
   },
   "outputs": [],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFw2z6ElMZMX"
   },
   "source": [
    "### Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rd7890Z6zLr"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = ImdbDataset(\n",
    "    reviews=df.review.to_numpy(),\n",
    "    targets=df.sentiment.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVU8o6i569ly"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aC5D5Dh8J5w9"
   },
   "source": [
    "### Loading the Pre-trained XLNet model for sequence classification from huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5mC8v6i7VH1"
   },
   "outputs": [],
   "source": [
    "from transformers import XLNetForSequenceClassification\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels = 2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYsVoULvfmvD"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpn2sTTMK_zL"
   },
   "source": [
    "### Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQ9Od31B9YJa"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2AtJcvwLV3x"
   },
   "source": [
    "### Sanity check with one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUoaMqjvKdym"
   },
   "outputs": [],
   "source": [
    "data = next(iter(val_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIUB5WJNKhBs"
   },
   "outputs": [],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "targets = data['targets'].to(device)\n",
    "print(input_ids.reshape(4,512).shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYsDR9leYb4Z"
   },
   "outputs": [],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCFLKXL0KmaA"
   },
   "outputs": [],
   "source": [
    "outputs = model(input_ids.reshape(4,512), token_type_ids=None, attention_mask=attention_mask, labels=targets)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbyJNtqVkg4Y"
   },
   "outputs": [],
   "source": [
    "type(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg44cHnNLd3J"
   },
   "source": [
    "### Defining the training step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPnWttRNMArt"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "  \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].reshape(4,512).to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # preds = preds.cpu().detach().numpy()\n",
    "        _, prediction = torch.max(outputs[1], dim=1)\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        prediction = prediction.cpu().detach().numpy()\n",
    "        accuracy = metrics.accuracy_score(targets, prediction)\n",
    "\n",
    "        acc += accuracy\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        counter = counter + 1\n",
    "\n",
    "    return acc / counter, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4N2ktqT8LoDS"
   },
   "source": [
    "### Defining the evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_ZyoJ4qb-CB"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].reshape(4,512).to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "\n",
    "            _, prediction = torch.max(outputs[1], dim=1)\n",
    "            targets = targets.cpu().detach().numpy()\n",
    "            prediction = prediction.cpu().detach().numpy()\n",
    "            accuracy = metrics.accuracy_score(targets, prediction)\n",
    "\n",
    "            acc += accuracy\n",
    "            losses.append(loss.item())\n",
    "            counter += 1\n",
    "\n",
    "    return acc / counter, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AG1TE43LvkX"
   },
   "source": [
    "### Fine-tuning the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNSQSFkScp6f"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,     \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler, \n",
    "        len(df_train)\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} Train accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader, \n",
    "        device, \n",
    "        len(df_val)\n",
    "    )\n",
    "\n",
    "    print(f'Val loss {val_loss} Val accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), '/content/drive/My Drive/NLP/Sentiment Analysis Series/models/xlnet_model.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe08GjtNL-Sh"
   },
   "source": [
    "### Evaluation of the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqtQkz2yrZE3"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/content/drive/My Drive/NLP/Sentiment Analysis Series/models/xlnet_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26k0PMpdy1QT"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHINzx6ezSD0"
   },
   "outputs": [],
   "source": [
    "test_acc, test_loss = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "print('Test Accuracy :', test_acc)\n",
    "print('Test Loss :', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBWsLq4yzubR"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    \n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "\n",
    "            texts = d[\"review_text\"]\n",
    "            input_ids = d[\"input_ids\"].reshape(4,512).to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            \n",
    "            _, preds = torch.max(outputs[1], dim=1)\n",
    "\n",
    "            probs = F.softmax(outputs[1], dim=1)\n",
    "\n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(probs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwCQaTFH5KWy"
   },
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wviSSrIP5Pvl"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdrnJkm-MGvv"
   },
   "source": [
    "### Custom prediction function on raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIxCoTjo6tWM"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    review_text = text\n",
    "\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=False,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = pad_sequences(encoded_review['input_ids'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
    "    input_ids = input_ids.astype(dtype = 'int64')\n",
    "    input_ids = torch.tensor(input_ids) \n",
    "\n",
    "    attention_mask = pad_sequences(encoded_review['attention_mask'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
    "    attention_mask = attention_mask.astype(dtype = 'int64')\n",
    "    attention_mask = torch.tensor(attention_mask) \n",
    "\n",
    "    input_ids = input_ids.reshape(1,512).to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    outputs = outputs[0][0].cpu().detach()\n",
    "\n",
    "    probs = F.softmax(outputs, dim=-1).cpu().detach().numpy().tolist()\n",
    "    _, prediction = torch.max(outputs, dim =-1)\n",
    "\n",
    "    print(\"Positive score:\", probs[1])\n",
    "    print(\"Negative score:\", probs[0])\n",
    "    print(f'Review text: {review_text}')\n",
    "    print(f'Sentiment  : {class_names[prediction]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UlPGZgNFV1j"
   },
   "outputs": [],
   "source": [
    "text = \"Movie is the worst one I have ever seen!! The story has no meaning at all\"\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oO0OhNoF4wo"
   },
   "outputs": [],
   "source": [
    "text = \"This is the best movie I have ever seen!! The story is such a motivation\"\n",
    "predict_sentiment(text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment Analysis Series part-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
